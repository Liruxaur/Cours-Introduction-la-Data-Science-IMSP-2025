{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abddf9af",
   "metadata": {},
   "source": [
    "# Notebook 3 — Comprendre les algorithmes d'apprentissage automatiques + SVM & clustering hiérarchique\n",
    "\n",
    "Ce notebook est en **deux parties** :\n",
    "1. **Construire k-means étape par étape** (en « briques ») pour comprendre ce qui se passe :\n",
    "   - choix de la **loss** (fonction objectif)\n",
    "   - assignation aux centres\n",
    "   - mise à jour des centroïdes\n",
    "   - critère d'arrêt\n",
    "2. **SVM + clustering hiérarchique** : réglage des hyperparamètres, métriques, accuracy, matrice de confusion.\n",
    "\n",
    " **Datasets utilisés (accessibles sans téléchargement)** :\n",
    "- `make_blobs` (données synthétiques générées par scikit-learn) pour le clustering\n",
    "- `load_digits` (dataset intégré à scikit-learn) pour la classification SVM\n",
    "\n",
    "> Certaines cellules sont à compléter : cherchez `TODO`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & config ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs, load_digits\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a84d8",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 1 — k-means « from scratch »\n",
    "\n",
    "### 1) Génération d'un dataset de clustering\n",
    "On génère des points en 2D avec `make_blobs` pour visualiser facilement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset synthétique (clustering)\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=400,\n",
    "    centers=4,\n",
    "    cluster_std=1.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], s=15)\n",
    "plt.title(\"Données (make_blobs) — pas de labels pour k-means\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()\n",
    "\n",
    "print(\"X shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ce327",
   "metadata": {},
   "source": [
    "### 2) Définir la loss (fonction objectif) de k-means\n",
    "La version classique de k-means minimise la **somme des distances au carré** aux centroïdes :\n",
    "\\[\n",
    "J = \\sum_{i=1}^{n} \\|x_i - \\mu_{c(i)}\\|^2\n",
    "\\]\n",
    "où $c(i)$ est le cluster assigné au point $x_i$.\n",
    "\n",
    "**À faire :** compléter la fonction `kmeans_loss`.\n",
    "- Entrées : `X` (n×d), `centroids` (k×d), `labels` (n)\n",
    "- Sortie : un scalaire (loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_loss(X, centroids, labels):\n",
    "    \"\"\"Somme des distances au carré aux centroïdes (inertia).\n",
    "    TODO: implémenter la loss.\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1) pour chaque point i, récupérer son centroïde centroids[labels[i]]\n",
    "    # 2) calculer ||x_i - mu||^2 et sommer\n",
    "    loss = ...  # TODO\n",
    "    return loss\n",
    "\n",
    "# --- mini test (doit tourner après votre implémentation) ---\n",
    "# centroids_test = X[:4]\n",
    "# labels_test = np.zeros(X.shape[0], dtype=int)\n",
    "# print(kmeans_loss(X, centroids_test, labels_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556099bb",
   "metadata": {},
   "source": [
    "### 3) Étape A — Initialisation des centroïdes\n",
    "Plusieurs stratégies :\n",
    "- choix aléatoire de points (simple)\n",
    "- k-means++ (meilleur, mais plus avancé)\n",
    "\n",
    "**À faire :** initialiser `k` centroïdes en sélectionnant `k` points au hasard dans `X`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_centroids_random(X, k, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    # TODO: tirer k indices sans remise (replace=False)\n",
    "    idx = ...  # TODO\n",
    "    return X[idx]\n",
    "\n",
    "k = 4\n",
    "centroids = init_centroids_random(X, k)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], s=15)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=120, marker=\"X\")\n",
    "plt.title(\"Initialisation aléatoire des centroïdes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40744df",
   "metadata": {},
   "source": [
    "### 4) Étape B — Assignation (E-step)\n",
    "On assigne chaque point au centroïde **le plus proche**.\n",
    "\n",
    "**À faire :** compléter `assign_labels`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_labels(X, centroids):\n",
    "    \"\"\"Retourne labels (n,) : l'indice du centroïde le plus proche pour chaque point.\"\"\"\n",
    "    # TODO:\n",
    "    # 1) calculer les distances au carré entre chaque point et chaque centroïde\n",
    "    #    astuce: broadcasting -> (n,1,d) - (1,k,d) -> (n,k,d)\n",
    "    # 2) argmin sur l'axe des clusters\n",
    "    labels = ...  # TODO\n",
    "    return labels\n",
    "\n",
    "# labels = assign_labels(X, centroids)\n",
    "# print(np.bincount(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4616b8",
   "metadata": {},
   "source": [
    "### 5) Étape C — Mise à jour des centroïdes (M-step)\n",
    "Chaque centroïde devient la **moyenne** des points qui lui sont assignés.\n",
    "\n",
    "**À faire :** compléter `update_centroids`.\n",
    "- Attention au cas où un cluster se vide (aucun point assigné) : stratégie simple = réinitialiser au hasard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef9ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroids(X, labels, k, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    d = X.shape[1]\n",
    "    new_centroids = np.zeros((k, d))\n",
    "\n",
    "    for j in range(k):\n",
    "        cluster_points = X[labels == j]\n",
    "        if len(cluster_points) == 0:\n",
    "            # TODO: cluster vide -> réinitialiser avec un point aléatoire\n",
    "            new_centroids[j] = ...  # TODO\n",
    "        else:\n",
    "            # TODO: moyenne des points\n",
    "            new_centroids[j] = ...  # TODO\n",
    "\n",
    "    return new_centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee7a73",
   "metadata": {},
   "source": [
    "### 6) Boucle k-means complète + critère d'arrêt\n",
    "On répète : assignation → mise à jour, jusqu'à convergence.\n",
    "\n",
    "**Critères d'arrêt possibles :**\n",
    "- nombre max d'itérations\n",
    "- déplacement des centroïdes < tolérance\n",
    "- amélioration de la loss < tolérance\n",
    "\n",
    "**À faire :** compléter `kmeans_fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ea9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_fit(X, k, max_iter=50, tol=1e-4, random_state=42):\n",
    "    centroids = init_centroids_random(X, k, random_state=random_state)\n",
    "    prev_loss = None\n",
    "    history = []\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        # TODO: assigner labels\n",
    "        labels = ...  # TODO\n",
    "\n",
    "        # TODO: calculer la loss\n",
    "        loss = ...  # TODO\n",
    "        history.append(loss)\n",
    "\n",
    "        # TODO: mettre à jour centroids\n",
    "        new_centroids = ...  # TODO\n",
    "\n",
    "        # critère d'arrêt (loss)\n",
    "        if prev_loss is not None and abs(prev_loss - loss) < tol:\n",
    "            print(f\"Convergence: it={it}, loss={loss:.3f}\")\n",
    "            centroids = new_centroids\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "        prev_loss = loss\n",
    "\n",
    "    return centroids, labels, history\n",
    "\n",
    "# --- Exécution protégée ---\n",
    "try:\n",
    "    centroids_f, labels_f, history = kmeans_fit(X, k=4, max_iter=80, tol=1e-3, random_state=42)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history, marker=\"o\")\n",
    "    plt.xlabel(\"itération\")\n",
    "    plt.ylabel(\"loss (inertia)\")\n",
    "    plt.title(\"Évolution de la loss de k-means\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels_f, s=15)\n",
    "    plt.scatter(centroids_f[:, 0], centroids_f[:, 1], s=140, marker=\"X\")\n",
    "    plt.title(\"Résultat k-means (from scratch)\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Complétez les TODO de la Partie 1 pour exécuter k-means:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3cf2f9",
   "metadata": {},
   "source": [
    "### 7) Comparaison rapide avec `sklearn.cluster.KMeans`\n",
    "Quand votre implémentation marche, comparez le résultat (qualitativement) avec `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e69aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=4, n_init=20, random_state=42)\n",
    "labels_sk = km.fit_predict(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels_sk, s=15)\n",
    "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=140, marker=\"X\")\n",
    "plt.title(\"k-means (scikit-learn)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc62ea5",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 2 — SVM, clustering hiérarchique, hyperparamètres et métriques\n",
    "\n",
    "### 8) Dataset de classification : `load_digits`\n",
    "On charge des images 8×8 de chiffres (0..9). C’est un dataset classique et intégré à scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a02e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data          # (n_samples, 64)\n",
    "y = digits.target        # (n_samples,)\n",
    "print(\"X:\", X.shape, \" y:\", y.shape)\n",
    "print(\"Classes:\", np.unique(y))\n",
    "\n",
    "# Visualiser quelques exemples\n",
    "fig, axes = plt.subplots(2, 6, figsize=(10, 3))\n",
    "for ax, idx in zip(axes.ravel(), range(12)):\n",
    "    ax.imshow(digits.images[idx], cmap=\"gray\")\n",
    "    ax.set_title(str(y[idx]))\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f55d5b4",
   "metadata": {},
   "source": [
    "### 9) Split train/test + baseline SVM\n",
    "SVM est sensible à l'échelle : on utilise un `StandardScaler`.\n",
    "\n",
    "**À faire :**\n",
    "- compléter les paramètres de `train_test_split`\n",
    "- entraîner un SVM RBF (`SVC`) et calculer accuracy + matrice de confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeff292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# TODO: split reproductible (test_size 0.2, random_state 42, stratify y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=...,      # TODO\n",
    "    random_state=...,   # TODO\n",
    "    stratify=...        # TODO\n",
    ")\n",
    "\n",
    "svm_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svc\", SVC(\n",
    "        kernel=\"rbf\",\n",
    "        C=...,          # TODO: ex 10\n",
    "        gamma=...       # TODO: ex \"scale\" ou 0.01\n",
    "    ))\n",
    "])\n",
    "\n",
    "try:\n",
    "    svm_pipe.fit(X_train, y_train)\n",
    "    y_pred = svm_pipe.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy test:\", acc)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title(\"Matrice de confusion — SVM baseline\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nRapport de classification:\\n\", classification_report(y_test, y_pred))\n",
    "except Exception as e:\n",
    "    print(\" Complétez les TODO SVM (C, gamma, split) pour exécuter:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754ebdc",
   "metadata": {},
   "source": [
    "### 10) Réglage des hyperparamètres SVM (GridSearchCV)\n",
    "On cherche de bons hyperparamètres `C` et `gamma` pour un noyau RBF.\n",
    "\n",
    "**À faire :**\n",
    "- compléter la grille `param_grid`\n",
    "- lancer `GridSearchCV`\n",
    "- évaluer le meilleur modèle sur le test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"svc__C\": [...],       # TODO: ex [0.1, 1, 10, 100]\n",
    "    \"svc__gamma\": [...]    # TODO: ex [0.001, 0.01, 0.1, \"scale\"]\n",
    "}\n",
    "\n",
    "svm = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svc\", SVC(kernel=\"rbf\"))\n",
    "])\n",
    "\n",
    "try:\n",
    "    gs = GridSearchCV(\n",
    "        svm,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Meilleurs hyperparamètres:\", gs.best_params_)\n",
    "    print(\"Meilleur score CV:\", gs.best_score_)\n",
    "\n",
    "    best_model = gs.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy test (best):\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    ConfusionMatrixDisplay(cm).plot()\n",
    "    plt.title(\"Matrice de confusion — SVM (meilleur modèle)\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\" Complétez la grille param_grid (TODO) pour lancer GridSearchCV:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29124e4",
   "metadata": {},
   "source": [
    "### 11) Clustering hiérarchique (Agglomerative Clustering)\n",
    "On revient sur les données 2D `make_blobs` pour visualiser.\n",
    "\n",
    "**Hyperparamètres importants :**\n",
    "- `n_clusters`\n",
    "- `linkage` : 'ward', 'complete', 'average', 'single'\n",
    "- `metric` (selon version) : distance utilisée\n",
    "\n",
    "**À faire :**\n",
    "- essayer plusieurs linkages et comparer visuellement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# On standardise pour que les distances soient comparables (souvent utile)\n",
    "Xc = StandardScaler().fit_transform(X)\n",
    "\n",
    "configs = [\n",
    "    # TODO: testez différents linkages\n",
    "    {\"n_clusters\": 4, \"linkage\": \"ward\"},\n",
    "    {\"n_clusters\": 4, \"linkage\": \"complete\"},\n",
    "    {\"n_clusters\": 4, \"linkage\": \"average\"},\n",
    "    {\"n_clusters\": 4, \"linkage\": \"single\"},\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    model = AgglomerativeClustering(**cfg)\n",
    "    labels = model.fit_predict(Xc)\n",
    "\n",
    "    sil = silhouette_score(Xc, labels)\n",
    "    plt.figure()\n",
    "    plt.scatter(Xc[:, 0], Xc[:, 1], c=labels, s=15)\n",
    "    plt.title(f\"Agglomératif — {cfg} | silhouette={sil:.3f}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883beff",
   "metadata": {},
   "source": [
    "### 12) Comparer (optionnel) : métriques de clustering si labels disponibles\n",
    "Sur `make_blobs`, on a `y_true` (labels générés). Ce n'est pas 'réel' en non supervisé, mais ça permet d'évaluer.\n",
    "\n",
    "**À faire :** calculer ARI (Adjusted Rand Index) pour comparer k-means sklearn vs hiérarchique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dfae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "Xc = StandardScaler().fit_transform(X)\n",
    "\n",
    "km = KMeans(n_clusters=4, n_init=20, random_state=42)\n",
    "labels_km = km.fit_predict(Xc)\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=4, linkage=\"ward\")\n",
    "labels_agg = agg.fit_predict(Xc)\n",
    "\n",
    "# TODO: calculez ARI pour les deux\n",
    "ari_km = ...   # TODO\n",
    "ari_agg = ...  # TODO\n",
    "\n",
    "print(\"ARI k-means:\", ari_km)\n",
    "print(\"ARI agglomératif:\", ari_agg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d660d",
   "metadata": {},
   "source": [
    "---\n",
    "## Questions / À rendre\n",
    "1. Dans votre k-means from scratch :\n",
    "   - quelle loss avez-vous implémentée ?\n",
    "   - quel critère d'arrêt utilisez-vous ?\n",
    "   - que se passe-t-il si un cluster se vide ?\n",
    "2. SVM : comment `C` et `gamma` influencent-ils la frontière de décision ? (intuition)\n",
    "3. Comparez la matrice de confusion avant/après GridSearchCV : quels chiffres sont les plus confondus ?\n",
    "4. Clustering hiérarchique : quel linkage donne la meilleure silhouette sur ce dataset ? Pourquoi ?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
