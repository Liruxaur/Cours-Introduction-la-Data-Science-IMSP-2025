{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d3de92",
   "metadata": {},
   "source": [
    "# Notebook 2 — Modèles de bases (classification, clustering, réduction de dimension)\n",
    "\n",
    "**Objectifs :**\n",
    "- Utiliser **Régression Logistique** (classification)\n",
    "- Utiliser **k-NN** (classification)\n",
    "- Utiliser **k-means** (clustering)\n",
    "- Utiliser **ACP / PCA** (réduction de dimension)\n",
    "- Évaluer les modèles et diagnostiquer **underfitting / overfitting**\n",
    "\n",
    "> Ce notebook est volontairement **guidé** : certaines lignes sont à compléter (TODO) pour vous amener à raisonner.\n",
    "\n",
    "---\n",
    "## Consignes\n",
    "- Lisez les cellules markdown (explications) puis complétez les cellules de code **aux endroits marqués `TODO`**.\n",
    "- Tant que vous n'avez pas complété, certaines cellules **sautent** l'exécution pour éviter les erreurs.\n",
    "- Travaillez proprement : variables claires, commentaires, et interprétation des résultats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f8fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & config ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a2d6f6",
   "metadata": {},
   "source": [
    "## 1) Dataset de classification (binaire) : Breast Cancer Wisconsin\n",
    "On va utiliser un dataset classique (déjà inclus dans scikit-learn) pour entraîner des modèles de **classification**.\n",
    "\n",
    "**Pourquoi ce dataset ?**\n",
    "- Beaucoup d'exemples\n",
    "- Variables numériques (facile à normaliser)\n",
    "- Deux classes (bénin / malin) → parfait pour la régression logistique et k-NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "342c2d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (569, 30)\n",
      "Classes y: {1: 357, 0: 212}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"target\")  # 0/1\n",
    "\n",
    "print(\"Shape X:\", X.shape)\n",
    "print(\"Classes y:\", y.value_counts().to_dict())\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07be88",
   "metadata": {},
   "source": [
    "### 1.1 Split train / test\n",
    "On sépare le dataset en **train** et **test** pour estimer la performance sur des données non vues.\n",
    "\n",
    "À compléter :\n",
    "- `test_size` (ex : 0.2)\n",
    "- `random_state` (ex : 42)\n",
    "- `stratify=y` (important en classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f2f6fc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'test_size' parameter of train_test_split must be a float in the range (0.0, 1.0), an int in the range [1, inf) or None. Got Ellipsis instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# TODO: complétez les paramètres pour un split reproductible et stratifié\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# TODO\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# TODO\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# TODO\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain:\u001b[39m\u001b[33m\"\u001b[39m, X_train.shape, \u001b[33m\"\u001b[39m\u001b[33m Test:\u001b[39m\u001b[33m\"\u001b[39m, X_test.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:208\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m to_ignore += [\u001b[33m\"\u001b[39m\u001b[33mself\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    206\u001b[39m params = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params.arguments.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__qualname__\u001b[39;49m\n\u001b[32m    210\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:98\u001b[39m, in \u001b[36mvalidate_parameter_constraints\u001b[39m\u001b[34m(parameter_constraints, params, caller_name)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m     constraints_str = (\n\u001b[32m     94\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:-\u001b[32m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[32m     99\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'test_size' parameter of train_test_split must be a float in the range (0.0, 1.0), an int in the range [1, inf) or None. Got Ellipsis instead."
     ]
    }
   ],
   "source": [
    "# TODO: complétez les paramètres pour un split reproductible et stratifié\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=...,          # TODO\n",
    "    random_state=...,       # TODO\n",
    "    stratify=...            # TODO\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5da634",
   "metadata": {},
   "source": [
    "## 2) Régression Logistique\n",
    "La **régression logistique** est un modèle linéaire pour la classification. On l'utilise souvent comme baseline.\n",
    "\n",
    "### 2.1 Pipeline (StandardScaler + LogisticRegression)\n",
    "On met un `StandardScaler` avant, car la régularisation et la descente d'optimisation dépendent de l'échelle des features.\n",
    "\n",
    "**À faire :**\n",
    "- Importer `LogisticRegression`\n",
    "- Créer un pipeline `scaler + modèle`\n",
    "- Entraîner (`fit`) puis prédire (`predict`) et évaluer (`accuracy`, matrice de confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da53fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# TODO: choisissez une valeur de C (ex: 1.0). Plus C est grand, moins il y a de régularisation.\n",
    "C_value = ...  # TODO\n",
    "\n",
    "log_reg_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(\n",
    "        C=C_value,\n",
    "        max_iter=500,\n",
    "        # TODO: si besoin, précisez solver=...\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- Exécution protégée : tant que C_value n'est pas rempli, on saute ---\n",
    "if C_value is ...:\n",
    "    print(\" Complétez C_value (TODO) pour entraîner la régression logistique.\")\n",
    "else:\n",
    "    log_reg_pipe.fit(X_train, y_train)\n",
    "    y_pred = log_reg_pipe.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy test:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nRapport:\\n\", classification_report(y_test, y_pred, target_names=data.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681c2c9",
   "metadata": {},
   "source": [
    "### 2.2 Overfitting / Underfitting via validation curve (paramètre C)\n",
    "Idée : comparer la performance **train** vs **validation** en faisant varier un hyperparamètre.\n",
    "\n",
    "- Si **train élevé** et **val faible** → *overfitting*\n",
    "- Si **train faible** et **val faible** → *underfitting*\n",
    "\n",
    "On va tracer une **validation curve** sur `C`.\n",
    "\n",
    "**À compléter :**\n",
    "- La liste de valeurs `C_grid`\n",
    "- L'appel à `validation_curve`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11037890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# TODO: grille de C (ex: np.logspace(-3, 3, 7))\n",
    "C_grid = ...  # TODO\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "if C_grid is ...:\n",
    "    print(\" Complétez C_grid (TODO) pour tracer la validation curve.\")\n",
    "else:\n",
    "    train_scores, val_scores = validation_curve(\n",
    "        pipe,\n",
    "        X_train, y_train,\n",
    "        param_name=\"model__C\",\n",
    "        param_range=C_grid,\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\"\n",
    "    )\n",
    "\n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    val_mean = val_scores.mean(axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.semilogx(C_grid, train_mean, marker=\"o\", label=\"train\")\n",
    "    plt.semilogx(C_grid, val_mean, marker=\"o\", label=\"validation\")\n",
    "    plt.xlabel(\"C (log scale)\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(\"Validation curve — Logistic Regression (C)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"→ Interprétez la zone où l'écart train/val est grand (overfitting) et où les deux sont faibles (underfitting).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf4787",
   "metadata": {},
   "source": [
    "## 3) k-NN (k plus proches voisins)\n",
    "k-NN prédit la classe d'un point en regardant les **k voisins** les plus proches (distance).\n",
    "\n",
    "**Point clé :** k-NN est très sensible à l'échelle des variables → on normalise.\n",
    "\n",
    "### 3.1 Entraîner et évaluer un k-NN\n",
    "**À compléter :**\n",
    "- Importer `KNeighborsClassifier`\n",
    "- Choisir une valeur `k`\n",
    "- Entraîner et évaluer sur test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = ...  # TODO: par ex 3, 5, 11...\n",
    "\n",
    "knn_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", KNeighborsClassifier(n_neighbors=k))\n",
    "])\n",
    "\n",
    "if k is ...:\n",
    "    print(\" Complétez k (TODO) pour entraîner k-NN.\")\n",
    "else:\n",
    "    knn_pipe.fit(X_train, y_train)\n",
    "    y_pred = knn_pipe.predict(X_test)\n",
    "    print(\"Accuracy test:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1176b1",
   "metadata": {},
   "source": [
    "### 3.2 Diagnostiquer under/overfitting en faisant varier k\n",
    "Intuition :\n",
    "- **k petit** → modèle très flexible (risque d'overfitting)\n",
    "- **k grand** → modèle plus lisse (risque d'underfitting)\n",
    "\n",
    "**À compléter :**\n",
    "- Une grille `k_values`\n",
    "- Calculer accuracy sur train et test (ou validation)\n",
    "- Tracer les courbes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d0afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = ...  # TODO: ex: range(1, 31, 2)\n",
    "\n",
    "if k_values is ...:\n",
    "    print(\" Complétez k_values (TODO).\")\n",
    "else:\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "\n",
    "    for k in k_values:\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"model\", KNeighborsClassifier(n_neighbors=k))\n",
    "        ])\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        # TODO: calculez les accuracies train et test\n",
    "        y_pred_train = ...  # TODO\n",
    "        y_pred_test = ...   # TODO\n",
    "\n",
    "        train_acc.append(accuracy_score(y_train, y_pred_train))\n",
    "        test_acc.append(accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(list(k_values), train_acc, marker=\"o\", label=\"train\")\n",
    "    plt.plot(list(k_values), test_acc, marker=\"o\", label=\"test\")\n",
    "    plt.xlabel(\"k (n_neighbors)\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(\"Under/Overfitting — k-NN\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"→ Où est-ce que le modèle overfit ? Où est-ce qu'il underfit ?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac33ef3",
   "metadata": {},
   "source": [
    "## 4) Clustering : k-means (non supervisé)\n",
    "Ici, pas de labels `y` : on cherche à regrouper les points en **k clusters**.\n",
    "\n",
    "On va utiliser le dataset **Iris** (3 espèces) uniquement pour visualiser : k-means n'utilise PAS les espèces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e0078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "X_iris.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655fb2f2",
   "metadata": {},
   "source": [
    "### 4.1 Choisir k : méthode du coude (inertia)\n",
    "k-means minimise la somme des distances intra-cluster (inertia).\n",
    "On trace inertia en fonction de k et on cherche un 'coude'.\n",
    "\n",
    "**À compléter :**\n",
    "- Une grille de `k` (ex: 1..10)\n",
    "- Calculer `inertia_` pour chaque k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ac66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_grid = ...  # TODO: ex range(1, 11)\n",
    "\n",
    "if k_grid is ...:\n",
    "    print(\" Complétez k_grid (TODO).\")\n",
    "else:\n",
    "    inertia = []\n",
    "    for k in k_grid:\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "        km.fit(X_iris)\n",
    "        inertia.append(km.inertia_)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(list(k_grid), inertia, marker=\"o\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"inertia\")\n",
    "    plt.title(\"Méthode du coude — k-means\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"→ Choisissez un k raisonnable d'après le coude.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d37d6c3",
   "metadata": {},
   "source": [
    "### 4.2 Qualité de clustering : silhouette score\n",
    "Le **silhouette score** (entre -1 et 1) évalue si les clusters sont bien séparés.\n",
    "\n",
    "**À compléter :**\n",
    "- Importer `silhouette_score`\n",
    "- Calculer le score pour plusieurs k (k>=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "k_grid2 = ...  # TODO: ex range(2, 11)\n",
    "\n",
    "if k_grid2 is ...:\n",
    "    print(\" Complétez k_grid2 (TODO).\")\n",
    "else:\n",
    "    scores = []\n",
    "    for k in k_grid2:\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "        labels = km.fit_predict(X_iris)\n",
    "        scores.append(silhouette_score(X_iris, labels))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(list(k_grid2), scores, marker=\"o\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"silhouette score\")\n",
    "    plt.title(\"Silhouette score — k-means\")\n",
    "    plt.show()\n",
    "\n",
    "    best_k = list(k_grid2)[int(np.argmax(scores))]\n",
    "    print(\"Meilleur k (selon silhouette):\", best_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a3cf69",
   "metadata": {},
   "source": [
    "## 5) Réduction de dimension : ACP / PCA\n",
    "L'ACP (PCA) projette les données dans un espace de dimension plus faible en conservant un maximum de variance.\n",
    "\n",
    "**À quoi ça sert ?**\n",
    "- Visualisation (2D)\n",
    "- Réduction du bruit\n",
    "- Accélérer certains algorithmes\n",
    "\n",
    "On va appliquer PCA sur Iris, après standardisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f5272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardisation + PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "\n",
    "pipe_pca = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", pca)\n",
    "])\n",
    "\n",
    "X_2d = pipe_pca.fit_transform(X_iris)\n",
    "\n",
    "print(\"X_2d shape:\", X_2d.shape)\n",
    "print(\"Variance expliquée (2 comps):\", pca.explained_variance_ratio_.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385211b",
   "metadata": {},
   "source": [
    "### 5.1 Visualisation 2D (PCA)\n",
    "On colorie par les vraies espèces *uniquement pour comprendre* ; PCA n'utilise pas ces labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd92ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=iris.target)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Iris projeté en 2D via PCA\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523033d0",
   "metadata": {},
   "source": [
    "### 5.2 k-means après PCA\n",
    "On peut faire k-means sur les composantes PCA (2D) :\n",
    "- plus rapide\n",
    "- parfois plus séparé\n",
    "\n",
    "**À compléter :**\n",
    "- Choisir `k`\n",
    "- Entraîner k-means sur `X_2d`\n",
    "- Visualiser les clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf50175",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = ...  # TODO: essayez 2, 3, 4...\n",
    "\n",
    "if k is ...:\n",
    "    print(\" Complétez k (TODO) pour k-means sur PCA.\")\n",
    "else:\n",
    "    km = KMeans(n_clusters=k, n_init=20, random_state=42)\n",
    "    labels = km.fit_predict(X_2d)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels)\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.title(f\"k-means (k={k}) sur projection PCA 2D\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879649c",
   "metadata": {},
   "source": [
    "## 6)  Cross-validation sur les modèles supervisés\n",
    "Au lieu d'un seul split train/test, on estime la performance avec une **cross-validation**.\n",
    "\n",
    "**À compléter :**\n",
    "- Calculer `cross_val_score` pour la régression logistique et pour k-NN\n",
    "- Comparer moyenne et écart-type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc30753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: choisissez un nombre de folds (cv)\n",
    "cv = ...  # TODO: ex 5\n",
    "\n",
    "if cv is ...:\n",
    "    print(\" Complétez cv (TODO).\")\n",
    "else:\n",
    "    # Logistic regression\n",
    "    log_reg = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(max_iter=500))\n",
    "    ])\n",
    "    scores_lr = cross_val_score(log_reg, X, y, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "    # k-NN (choisissez k)\n",
    "    k = ...  # TODO\n",
    "    if k is ...:\n",
    "        print(\" Complétez k (TODO) pour la CV de k-NN.\")\n",
    "    else:\n",
    "        knn = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"model\", KNeighborsClassifier(n_neighbors=k))\n",
    "        ])\n",
    "        scores_knn = cross_val_score(knn, X, y, cv=cv, scoring=\"accuracy\")\n",
    "\n",
    "        print(f\"LogReg CV accuracy: {scores_lr.mean():.3f} ± {scores_lr.std():.3f}\")\n",
    "        print(f\"kNN(k={k}) CV accuracy: {scores_knn.mean():.3f} ± {scores_knn.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899690b",
   "metadata": {},
   "source": [
    "---\n",
    "## À rendre / Questions\n",
    "1. Régression logistique : quelle valeur de **C** donne le meilleur compromis train/val ? Pourquoi ?\n",
    "2. k-NN : pour quelles valeurs de **k** observe-t-on overfitting / underfitting ?\n",
    "3. k-means : quel k choisissez-vous (coude vs silhouette) ? Comparez.\n",
    "4. PCA : quelle proportion de variance est expliquée par 2 composantes ? Est-ce suffisant ?\n",
    "5. (Bonus) Comparez les performances CV de LogReg vs k-NN.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
